---
title: "STAT946 Assignment 2"
author: |
  | JunYong Tong
  | **University of Waterloo**
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
biblio-style: apalike
link-citations: true
bibliography: []
output: 
  bookdown::gitbook:
    fig_caption: yes
    includes:
      in_header: preamble.html
---


# Introduction Outline

The main take away from this assignment are:   

- Stick-breaking representation of DPMM with large number of cluster initialization
- A heuristic method to merge/prune extra cluster
- A method to check posterior distributions by sampling from it in Pyro
- Some tips and tricks on using Pyro optimizer for Variational Inference

## Problem Statement

Let $G_0$ be Normal with Inverse-Gamma, and $\alpha > 0$ be a concentration parameter. Consider the Dirichlet Process Mixture Model (DPMM):

\begin{align}
G &\sim \mbox{DP}\{G_0, \alpha\} \nonumber \\
\tth_1, \ldots, \tth_n &\sim G (\#eq:dpmm-def) \\
\YY_i \mid \tth_i &\ind f(\yy \mid \tth_i) \nonumber
\end{align}

### SB DPMM

We are able to construct DPMM through the stick breaking process as follows: 

1. Draw $y_1, y_2, \ldots \iid G_0(y)$
2. Draw $\beta_1, \beta_2, \ldots \iid \mbox{Beta}(1,\alpha)$ and let $w_k = \beta_k \prod_{i=1}^{k-1}(1-\beta_i)$.
3. Let $G(y) = \sum_{k}w_k \delta_{y_k}$, $G(y)$ is drawn from $DP(\alpha, G_0)$.

```{python, eval=F}
def model(self, data=None, batch_size=None):
    with pyro.plate("beta_plate", self.T - 1):
        beta = pyro.sample("beta", dist.Beta(1, self.alpha))

    with pyro.plate("mu_plate", self.T):
        mu_sd = pyro.sample("musd", dist.InverseGamma(torch.ones_like(self.sd_q1),
                                                      torch.ones_like(self.sd_q2)).to_event(1))
        mu_c = pyro.sample("mu", dist.Normal(self.mu_c,
                                             mu_sd*torch.ones_like(self.mu_c)).to_event(1))

    with pyro.plate("data", size=self.num_obs, subsample=batch_size):
        ys = pyro.sample(f"cat", dist.Categorical(mix_weights(beta)), infer={"enumerate": "parallel"})
        pyro.sample(f"obs", dist.Normal(mu_c[ys], mu_sd[ys]).to_event(1), obs=data[batch_size])

```


We use variational distribution to approximate the posterior.

```{python, eval=F}
def guide(self, data=None, batch_size=None):

    alpha_q = pyro.param('alpha_q', self.alpha_q, constraint=constraints.positive)
    phi = pyro.param('pi_q', self.pi_c, constraint=constraints.simplex)
    mu_q = pyro.param("mu_q", self.mu_c)
    sd_q1 = pyro.param('sd_q1', self.sd_q1)
    sd_q2 = pyro.param('sd_q2', self.sd_q2)

    # pyro.module("predictor", self.pred)
    with pyro.plate("beta_plate", self.T - 1):
        f_beta = pyro.sample("beta", dist.Beta(torch.ones(self.T - 1), alpha_q))

    with pyro.plate("mu_plate", self.T):
        mu_sd = pyro.sample("musd", dist.InverseGamma(sd_q1, sd_q2).to_event(1))
        mu_c = pyro.sample("mu", dist.Normal(mu_q, mu_sd).to_event(1))

    with pyro.plate("data", size=self.num_obs, subsample=batch_size):
        # phi = self.pred(data[batch_size])
        # f_cat = pyro.sample(f"cat", dist.Categorical(logits=phi))
        f_cat = pyro.sample(f"cat", dist.Categorical(phi[batch_size]))
```

## Data simulation for inference

The main point is to come up with simple yet non-trivial example.

```{r, echo=F, fig.align='center', fig.cap="Data Simulated from three Normal distribution."}
knitr::include_graphics("sim_data.png")
```

## Variational Inference

- Plot the fitted posterior
- Posterior predictive check in Pyro
- Tricks in making the VI converge

### Merge and Prune extra components

- Present heuristics to prune. Too little weight.
- Present heuristics to merge. Merge small KL.


